
학습 동기
기존의 데이터베이스로는 처리하기 힘든 방대한 양의 데이터가 끊임없이 생성되고 있다.
빅 데이터에서 가치를 생성할 수 있는 전문가가 필요하다.

학습 목표
- 여러 유형의 데이터에 대해 마이닝하는 방법을 공부한다.
- 빅 데이터에 적용가능한 다양한 도구들의 사용법을 익힌다.
- 빅데이터로 현실의 문제들을 해결하는 방법을 공부한다.


과제
Colab에서 PySpark를 사용하는 과제

마이너리티리포트 [메이저리티리포트 - 마이너리티리포트]

Data contains value and knowlegde

데이터를 이용한 범죄예방 ? 사생활 침해?

---
Data Mining

지식 데이터로 추출하기 위해

- 저장 [데이터베이스]

- 운영 [시스템]

- 분석 < 이 강의에서 다룰 내용

매우 큰 데이터 셋에서 actionable(실용가능한)한 정보를 추출
[극도의 과장, 공포, 흥미의 대상이다]

큰 데이터 처리 - 병렬화가 필수적이다.

---
데이터 마이닝 방법

-기술적인 방법 [문장을 기술하다]
인간의 이해를 바탕으로 분석

- 예측적인 방법
인자들을 방법으로 모르는값이나 미래의 값들을 예측

---

다른 타입의 데이터를 mine 하는법

다양한 computation 모델
-MapReduce
-Streams and online algotithms
-Single machine in-memory

현실 세계 문제 해결
- 추천 시스템
- 장바구니 분석
- 스팸 방어
- 중복 문서 탐지

다양한 "도구"들
-Linear algebra
-Optimization
-Dynamic prohramming
-Hashing

---

Large-scale Computing

commodity hardware [상품 하드웨어]

기계 고장 문제
컴퓨터들이 망가진다 하나의 서버의 수명 3년
1000개의 서버 하루에 한개씩 망가짐
1M 개 면 1000개의 머신이 매일 망가짐

---
아이디어와 솔루션

이슈 : 데이터를 카피할때 네트워크에서 시간이 걸림
생각 : 계산을 데이터로 가져오자 , 저장하자 파일들을 여러타임 신뢰할 수 있게

Spark/Hadoop 이 문제들에 주소한다.
저장소 인프라 - 파일 시스템 - Google GFS, Hadoop : HDFS
프로그래밍 모델 - MapReduce Spark

---
머신들을 묶어서 데이터 저장

분산형 파일 시스템
	전역 파일 네임스페이스를 제공

전통적인 사용 패턴
	큰 파일 (수백 기가. 수백 테라)
	데이터는 거의 업데이트 되지 않음
	읽히고 끝부분에 추가

---
 
병렬컴퓨팅

큰 스케일의 컴퓨팅

- 계산을 어떻게 분배시킬것인가?
- 분배된 프로그램들을 코딩하는 것을 어떻게 쉽게 할 것인가?
- 머신들의 실패(고장)

---
아이디어와 해결

이슈
-네트워크 실행 시간을 넘은 데이터 복사

아이디어
-데이터로 계산을 가져온다
신뢰도를 위해 파일들을 여러 시간에 걸쳐  저장한다 

Spark/ Hadoop : 이 문제를 해결

--------------
분산 파일 시스템

Chunk servers
- chunk는 덩어리
파일은 연속된 덩어리들로 나눠진다.
- 16~64 MB
- 서로 다른 위치에 저장되도록 한다. (서로 다른 컴퓨터(different racks))

Master node
- a.k.a - Hadoop's HDFS

파일 접근을 위한 클라이언트 라이브러리

신회할 수 있는 분산 파일 시스템
여러 chunks 로 나뉘어서 저장되기 때문에
각 청크는 복제된다 다른 머신으로

---
MapReduce 초기 분산 컴퓨팅 프로그래밍 모델
(분할 정복)

맵리듀스는 프로그래밍 스타일인데 무었을 위하냐?
1. 쉬운 병렬 프로그매밍
2. 하드웨어와 소프트웨어 고장에 대한  보이지않는 운영 
3. 매우 큰 스케일의 데이터의 쉬운 운영

맵 리듀스의 3가지 스템
Map
- 한 사용자가 입력한 Map Function을  각 인풋 요소로 적용한다.
	Mapper apllies the map funtion to a single element
Group by Key : 정렬, 섞기
- 시스템은 키를 통해 모든 키-값 쌍을 정렬한다 , 그리고 키-(값의 리스트) 쌍을 추출한다 

Reduce 
- 사용자가 만든 Reduce Funtion은 각 key-(값의 리스트) 쌍에게 적용된다 

---
복습

분산 컴퓨팅은 위험부담이 있기때문에
스파크/하둡을 동한 분산 저장으로 해결

맵리듀스
프로그래밍모델
하둡과 스파크에 구현되어있다.
3가지 단계 맵 - Group by Key - 리듀스 (사용자는 맵과 리듀스만 건드림

---
Map : 입력을 읽고 키-값 쌍의 집합을 제공한다.

Group by Key : 같은 키 (hash merges, shuffle, sort, partition)의 모든 쌍을 모은다.

Reduce : 키와 출력에 속하는 모든 값들을 모은다.

---
데이터에 대해 키와 값 쌍을 만들어냄
같은 키의 쌍을 모음
리듀서에서에서 적절하게 출력함

---
예제 : Word Counting

- 우리는 아주 방대한 문서를 가지고있다.
- 파일에 나타난 각 구별할만한 단어들의 수를 세어라

이것의 많은 어플리케이션:
-유명한 URL 을 찾기위해 웹 서버 로그를 분석
- 통계 기계 번역 :
큰 corpus 문서에서 매 5단어마다 발생하는 횟수를 셀 필요가 있다.

---
방대한 문서

[프로그래머]
Map : 단어 : 1
(the, 1)
(crew, 1)
(of , 1)
(space, 1)
...

[컴퓨터]
Group by key : 정렬
(crew, 1)
(crew, 1)
(space, 1)
(space, 1)
(space, 1)
...

[프로그래머]
Reduece : 병합, 출력
(crew, 2)
(space, 3)
(the, 3)
...

---
슈도 코드

map(key, value) :
# key : document name, value, text of the document for each word w in value:
emit(w, 1)

reduce(key, values):
# key : a word, value : an iterator over counts
result = 0
for each count v in values:
result += v
emit(key, result)

---
맵 리듀스 환경이 신경써주는 것
- 파티셔닝
- 스케쥴링
- 키값에 따른 그루핑 (병목현상 발생)
- 기계 고장에 대한 핸들링
- 기계끼리의 상호 커뮤니케이션

---
고장을 다루는 법

Map worker 고장
- 맵 워커가 망가졌다는것이 감지되면 일을 끝냈던지 하고있던지 상관하지 않고 무조건 
다른 워커에게 넘겨준다.
- 리듀스 워커들도 고장난 워커에대한 알림을 받는다.

Reduce worker 고장
- 리듀스 워커가 고장나면 끝난 작업에 대해서는 신경쓰지 않고 진행중인 작업만 다른
워커에게 넘겨준다.

---
복습

맵 : 각 정보들에 대해서 키-값으로 매핑한다
그룹 바이 키 : 키 값을 기준으로 값들을 모아줌
리듀스 : 프로그래머가 코딩한 것에 따라 키 값을 모아서 출력

---
맵 리듀스의 문제

맵 리듀스를 여러번 할때 마다 디스크에서 HDFS 읽고 쓰기의 작업이 많이 일어난다.
디스크의 I/O의 속도는 비교적 굉장히 느리고 오버헤드가 크다.

많은 프로그램들이 맵 리듀스로 정해지지 않는다.

요약하자면 맵 리듀스는 어울리지 않는다.

---
Data-Flow Systems 

맵 리듀스는 두가지 "rank" 작업을 활용한다.
Map과 Reduce

* Data-Flow Systems 는 투가지 방법을 제공한다.
1. 어떠한 rank던 작업이던 허용하낟.
2. Map과 Reduce 외의 다른 메소드도 허용한다.
- 데이터가 한방향으로 흐르기만한다면 중단된 시점부터 다시 시작할 수 있도록 하는 시스템

---
Spark : 가장 인기있는 Data-Flow System

맵 리듀스 모델에 제한 되지 않는 컴퓨팅 시스템
- 빠른 데이터 공유
- 디스크 저장을 피함
- 캐시 데이터를 사용해서 반복적인 쿼리에 유리
- 방향성이 있는 그래프를 사용(데이터 흐름의 역행을 피함)
- map 과 reduce 에 국한되지 않는 다양한 함수 사용가능

자바와 파이썬 사용 scala도 사용

주요 생성/아이디어 : Resilient Distributed Dataset (RDD)

---
RDD란?

분산되어 저장되는 데이터 셋
(key-value 형태를 일반화한것)

RDD에서는 key-value 형태를 강제하지 않는다.

Spark는 최대한 디스크를 안쓰려고하고 Caching 데이터를 통해 진행됨

RDD는 Hadoop이나 다른 RDD 변환 시스템으로 부터 생성된다.

---
Spark RDD Opertation

Transfromations : RDD를 만들어냄
- Transformation은 map, filter, join, union, intersection, distinct 가 포함되어있다.
- Lazy evaluation <> eager evaluation
스파크는 사용자의 작용이 없으면 수행하지 않는 Lazy evaluation을 사용[기타 다른 프로그램과 다른 모습]
(다른 프로그램은 출력의 작용이 있기전까지 연산을 해놓은 상태로 기다리지만 스파크는 출력 명령을 받고
연산을 시작함)

Action: 값과 데이터를 반환
- Action은 count, collect, reduce, save를 포함한다.

---
DAGS

방향성이 있고 사이클이 없어 역행하지 않는 그래프를 사용

- 일반적인 작업 그래프를 지원
- 파이프 라인을 사용
- 캐시를 쓸 수 있음(지역성, 재사용)
- 셔플의 병목현상을 피하도록 파티션을 사용한다.

---
DataFrame 
- 열과 행으로 이루어진 관계 데이터베이스 테이블

Dataset
- type-sage, 객체지향 프로그래밍 인터페이스

모두 Spark SQL 엔진에서 구성되며, RDD로 전환될 수 있다.

---
Useful Libraires from Spark 

Spark SQL

spark streaming - stram processing of live datastream
MLib - scalable machine learning
GraphX - graph manipulation

---
Spark vs Hadoop MapReduce

성능상으로는 스파크가 빠름

스파크는 메모리에서 data 접근 가능

MapReduce 는 단순하기때문에 다른 서비스들과 잘 작동

Spark는 메모리를 왕창 잡아먹어서 다른 서비스와 잘 작동이 안될 수 있음

프로그래머의 입장에서는 쉬운건 Spark (하이레벨 API)

---
Problems Suited for MapReduce

Example : Host size
우리가 큰 web corpus 를 가지고 있다고 가정해보자
metadata file을 보자 (URL, size, date,... ) 무수히 많은 데이터

각 호스트에게, 바이트의 최종 수를 찾는다
즉, 개인이 호스트하는 URL form page size의 합계

Example : Language Model
통계적 기계 번역 :
큰 corpus의 문서에서 문장이 발생할 때 5번째 단어마다 카운트를 해줘서 숫자를 세준다.

Example : Join By Map-Reduce
R(A,B) 와 S(B,C)를 조인해주는 것을 연산
각각의 파일을 저장해야함
(a,b) or (b,c) 쌍의 튜플

A  B     	 B  C 	A C
a1 b1  	 b2 c1	a3 c1
a2 b1 조인 b2 c2  =  a3 c2
a3 b2  	 b3 c3     	a4 c3
a4 b3
[같은거 끼리 묶어준다]
Map에서 key 값으로 B값을 써준다.
구분을 위해 속성값을 넣어줌
(b1, (a1, R))	(b2, (c1, S))	b1으로 묶이는거 없음
(b1, (a2, R))	(b2, (c2, S))	b2 -> a3 c1, c3 c2
(b2, (a3, R))	(b3, (c3, S))	b3 -> a4 c3
(b3, (a4, R))

-- 이렇게 말고 hash function을 사용할 수도 있음
[hash function h from B-value to 1...k]
Map 진행과정 :
각각은 입력한다 tuble R(a,b)를 key-value pair (b,(a,R))로
해시함수로 key를 넣은다음에 MapReduce 작업을 한다.

MapReduce는 랜덤하거나 불규칙한 데이터에 접근이 요구되는 문제에는 충분하지 않다.
- 그래프
- 상호독립적 데이터
 > 머신 러닝
 > 여러 페어의 아이템의 비교

---
input =>mappers =>[group by key] => reducers => output
---
맵리듀스 알고리즘의 비용을 측정하는 척도

1. Communication Cost = 모든 프로세스에서 일어난 I/O 작업
2. Elapsed comunnication cost = 모든 I/O 패스중 가장 큰 것
3. (Elapsed) computation cost = 모든 프로세스들의 총 실행 시간
* big-O notation은 효용성이 없다.

---
Spark : RDD

맵 리듀스나 Spark을 데이터 플로우 시스템으로 볼 수 있다.

데이터 플로우 : 데이터가 흘러갈 수 있는 길

데이터 플로우 처리 알고리즘이 2개가 있을 때
같은 치리 속도(양)의 결과가 나올때 [모든 프로세스에서 일어난 I/O 작업]
각 과정에서 어느정도의 처리양이 분산되어있는지가 중요하다. [모든 패스에서 일어난 각 작업]

---
Cost Measures
- Communication cost = 인풋 파일 사이즈 + 2x(맵 프로세스에서 리듀스 프로세스로 가는 파일 사이즈의 합) [그룹 바이 키때문에]
+ 리듀스 프로세스에서 출력된 사이즈의 합

- (Elapsed)Communication cost[(흘러간)] = 가장 큰 input + any 맵 프로세스의 출력 + 같은 것 any 리듀스 프로세스

---
What cost measure mean

-입출력이나 계산 둘중에 하나가 비용 측정을 결정한다 [둘 다 고려할 필요 x] [상황에 따라 바꿔가면서 사용]
-여러분이 사용하고 있는 클라우드 컴퓨팅에 지불하고 있는 비용을 나타낸다. (직접 서버 구동 vs 클라우드)(수지타산 비교)
- 흘러간 비용[elapsed cost]는 병렬화를 사용하는 벽시계 시간

---
map-reduce join의 비용
- total communication cost
= O(|R|+|S|+|R조인S|) - Big O는 대충 계산하는거
- Elapsed communication cost = O(s)
* Big O 관련내용은 시험에 내지 않을 겁니다

---
데이터에서 자주 등장하는 패턴 찾기 & 연관 법칙 찾기
---
연관 법칙 탐색

슈퍼 마켓에서 충분히 많은 고객들이 같이 사는 물건 찾기

"영수증을 봤을 때 A를 사는 사람들은 보통 B를 같이 사더라~"

---
장바구니 모델

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

출력:
규칙 발견 : 우유를 사면 콜라도 산다 / 기저귀, 우유를 사면 맥주도 산다

- 상당히 많은 아이템
- 상당히 많은 장바구니 내역(비교적적은 아이템 내용)

> 연관관계 발견
{x,y,z}를 사는 사람은 {v,w}를 사는 경향이 있다.

---
일반화

아이템/장바구니 > 단어/문서
아이템/장바구니 > 약/환자

* 꼭 장바구니에 한정되어서 사용되는 모델은 아니다
[아이템을의 연관관계를 찾기 위해 사용한다]
---
아이템/장바구니
문장/문서
환자/약&부작용

> 유의미한 연관관계를 찾아내 긍정적인 효과를 위해 사용할 수 있다.

---
개요

First : 정의
빈번한 아이템셋은 무엇인가?
연관관계는 무엇인가? : Confidence(신뢰도/확신), Support(지원), interestingness(흥미)

Second(Then) : 빈번한 아이템셋 찾기 알고리즘
- Finding frequent pairs 빈번한 쌍 찾기
- A-Priori algorithm 선행 알고리즘 / 빈발항목집합(frequent itemsets) 및 연관규칙분석을 위한 알고리즘
- PCY algorithm A-Priori 알고리즘에서, 첫번째 패스의 사용되지 않는 메모리 공간을 활용하는 알고리즘

---
빈발항목집합(frequent itemsets)

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

Support : 해당 아이템 셋을 품고 있는 장바구니를 찾아라!

Support of {맥주,빵}
>> 구매내역에서 맥주와 빵을 둘다 포함하는 장바구니 찾기 >> 2, 4
Support of {맥주,빵} = 2  || [2/5  = 0.4] < 이렇게 표기하기도함

Support threshold (s) : 이정도면 빈번하게 등장하는 구나라고 정의하는 한계점
Support 가 이 s 를 넘으면 빈번한 아이템셋으로 셀 수 있음

---
Association Rules (연관 규칙)

장바구니에 우유가 있으면 맥주가 있을 확률이 높다.
장바구니에 x가 있으면 y가 있을 확률이 높다.

> 이러한 규칙을 찾아내는 것 (연관있는, 흥미로운 규칙을 찾아내기)

Confidence(신뢰도/확신)은 I의 집합에 대해서 j에 대한 신뢰도를 나타냄

conf(I -> j) = support(I U j) / support(I)
> 여러가지 아이템들의 집합에서 J가 등장할때의 신뢰도는 해당 아이템이 등장한 횟수 중 I와 j가 같이 등장한 횟수

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

우유 ->콜라 의 신뢰도(confidence)
=  콜라와 우유의 support(콜라와 우유가 같이 등장한 횟수)/우유의 support(우유가 등장한 횟수) 
= 3/4 = 0.75

