학습 동기
기존의 데이터베이스로는 처리하기 힘든 방대한 양의 데이터가 끊임없이 생성되고 있다.
빅 데이터에서 가치를 생성할 수 있는 전문가가 필요하다.

학습 목표
- 여러 유형의 데이터에 대해 마이닝하는 방법을 공부한다.
- 빅 데이터에 적용가능한 다양한 도구들의 사용법을 익힌다.
- 빅데이터로 현실의 문제들을 해결하는 방법을 공부한다.


과제
Colab에서 PySpark를 사용하는 과제

마이너리티리포트 [메이저리티리포트 - 마이너리티리포트]

Data contains value and knowlegde

데이터를 이용한 범죄예방 ? 사생활 침해?

---
Data Mining

지식 데이터로 추출하기 위해

- 저장 [데이터베이스]

- 운영 [시스템]

- 분석 < 이 강의에서 다룰 내용

매우 큰 데이터 셋에서 actionable(실용가능한)한 정보를 추출
[극도의 과장, 공포, 흥미의 대상이다]

큰 데이터 처리 - 병렬화가 필수적이다.

---
데이터 마이닝 방법

-기술적인 방법 [문장을 기술하다]
인간의 이해를 바탕으로 분석

- 예측적인 방법
인자들을 방법으로 모르는값이나 미래의 값들을 예측

---

다른 타입의 데이터를 mine 하는법

다양한 computation 모델
-MapReduce
-Streams and online algotithms
-Single machine in-memory

현실 세계 문제 해결
- 추천 시스템
- 장바구니 분석
- 스팸 방어
- 중복 문서 탐지

다양한 "도구"들
-Linear algebra
-Optimization
-Dynamic prohramming
-Hashing

---

Large-scale Computing

commodity hardware [상품 하드웨어]

기계 고장 문제
컴퓨터들이 망가진다 하나의 서버의 수명 3년
1000개의 서버 하루에 한개씩 망가짐
1M 개 면 1000개의 머신이 매일 망가짐

---
아이디어와 솔루션

이슈 : 데이터를 카피할때 네트워크에서 시간이 걸림
생각 : 계산을 데이터로 가져오자 , 저장하자 파일들을 여러타임 신뢰할 수 있게

Spark/Hadoop 이 문제들에 주소한다.
저장소 인프라 - 파일 시스템 - Google GFS, Hadoop : HDFS
프로그래밍 모델 - MapReduce Spark

---
머신들을 묶어서 데이터 저장

분산형 파일 시스템
	전역 파일 네임스페이스를 제공

전통적인 사용 패턴
	큰 파일 (수백 기가. 수백 테라)
	데이터는 거의 업데이트 되지 않음
	읽히고 끝부분에 추가

---
 
병렬컴퓨팅

큰 스케일의 컴퓨팅

- 계산을 어떻게 분배시킬것인가?
- 분배된 프로그램들을 코딩하는 것을 어떻게 쉽게 할 것인가?
- 머신들의 실패(고장)

---
아이디어와 해결

이슈
-네트워크 실행 시간을 넘은 데이터 복사

아이디어
-데이터로 계산을 가져온다
신뢰도를 위해 파일들을 여러 시간에 걸쳐  저장한다 

Spark/ Hadoop : 이 문제를 해결

--------------
분산 파일 시스템

Chunk servers
- chunk는 덩어리
파일은 연속된 덩어리들로 나눠진다.
- 16~64 MB
- 서로 다른 위치에 저장되도록 한다. (서로 다른 컴퓨터(different racks))

Master node
- a.k.a - Hadoop's HDFS

파일 접근을 위한 클라이언트 라이브러리

신회할 수 있는 분산 파일 시스템
여러 chunks 로 나뉘어서 저장되기 때문에
각 청크는 복제된다 다른 머신으로

---
MapReduce 초기 분산 컴퓨팅 프로그래밍 모델
(분할 정복)

맵리듀스는 프로그래밍 스타일인데 무었을 위하냐?
1. 쉬운 병렬 프로그매밍
2. 하드웨어와 소프트웨어 고장에 대한  보이지않는 운영 
3. 매우 큰 스케일의 데이터의 쉬운 운영

맵 리듀스의 3가지 스템
Map
- 한 사용자가 입력한 Map Function을  각 인풋 요소로 적용한다.
	Mapper apllies the map funtion to a single element
Group by Key : 정렬, 섞기
- 시스템은 키를 통해 모든 키-값 쌍을 정렬한다 , 그리고 키-(값의 리스트) 쌍을 추출한다 

Reduce 
- 사용자가 만든 Reduce Funtion은 각 key-(값의 리스트) 쌍에게 적용된다 

---
복습

분산 컴퓨팅은 위험부담이 있기때문에
스파크/하둡을 동한 분산 저장으로 해결

맵리듀스
프로그래밍모델
하둡과 스파크에 구현되어있다.
3가지 단계 맵 - Group by Key - 리듀스 (사용자는 맵과 리듀스만 건드림

---
Map : 입력을 읽고 키-값 쌍의 집합을 제공한다.

Group by Key : 같은 키 (hash merges, shuffle, sort, partition)의 모든 쌍을 모은다.

Reduce : 키와 출력에 속하는 모든 값들을 모은다.

---
데이터에 대해 키와 값 쌍을 만들어냄
같은 키의 쌍을 모음
리듀서에서에서 적절하게 출력함

---
예제 : Word Counting

- 우리는 아주 방대한 문서를 가지고있다.
- 파일에 나타난 각 구별할만한 단어들의 수를 세어라

이것의 많은 어플리케이션:
-유명한 URL 을 찾기위해 웹 서버 로그를 분석
- 통계 기계 번역 :
큰 corpus 문서에서 매 5단어마다 발생하는 횟수를 셀 필요가 있다.

---
방대한 문서

[프로그래머]
Map : 단어 : 1
(the, 1)
(crew, 1)
(of , 1)
(space, 1)
...

[컴퓨터]
Group by key : 정렬
(crew, 1)
(crew, 1)
(space, 1)
(space, 1)
(space, 1)
...

[프로그래머]
Reduece : 병합, 출력
(crew, 2)
(space, 3)
(the, 3)
...

---
슈도 코드

map(key, value) :
# key : document name, value, text of the document for each word w in value:
emit(w, 1)

reduce(key, values):
# key : a word, value : an iterator over counts
result = 0
for each count v in values:
result += v
emit(key, result)

---
맵 리듀스 환경이 신경써주는 것
- 파티셔닝
- 스케쥴링
- 키값에 따른 그루핑 (병목현상 발생)
- 기계 고장에 대한 핸들링
- 기계끼리의 상호 커뮤니케이션

---
고장을 다루는 법

Map worker 고장
- 맵 워커가 망가졌다는것이 감지되면 일을 끝냈던지 하고있던지 상관하지 않고 무조건 
다른 워커에게 넘겨준다.
- 리듀스 워커들도 고장난 워커에대한 알림을 받는다.

Reduce worker 고장
- 리듀스 워커가 고장나면 끝난 작업에 대해서는 신경쓰지 않고 진행중인 작업만 다른
워커에게 넘겨준다.

---
복습

맵 : 각 정보들에 대해서 키-값으로 매핑한다
그룹 바이 키 : 키 값을 기준으로 값들을 모아줌
리듀스 : 프로그래머가 코딩한 것에 따라 키 값을 모아서 출력

---
맵 리듀스의 문제

맵 리듀스를 여러번 할때 마다 디스크에서 HDFS 읽고 쓰기의 작업이 많이 일어난다.
디스크의 I/O의 속도는 비교적 굉장히 느리고 오버헤드가 크다.

많은 프로그램들이 맵 리듀스로 정해지지 않는다.

요약하자면 맵 리듀스는 어울리지 않는다.

---
Data-Flow Systems 

맵 리듀스는 두가지 "rank" 작업을 활용한다.
Map과 Reduce

* Data-Flow Systems 는 투가지 방법을 제공한다.
1. 어떠한 rank던 작업이던 허용하낟.
2. Map과 Reduce 외의 다른 메소드도 허용한다.
- 데이터가 한방향으로 흐르기만한다면 중단된 시점부터 다시 시작할 수 있도록 하는 시스템

---
Spark : 가장 인기있는 Data-Flow System

맵 리듀스 모델에 제한 되지 않는 컴퓨팅 시스템
- 빠른 데이터 공유
- 디스크 저장을 피함
- 캐시 데이터를 사용해서 반복적인 쿼리에 유리
- 방향성이 있는 그래프를 사용(데이터 흐름의 역행을 피함)
- map 과 reduce 에 국한되지 않는 다양한 함수 사용가능

자바와 파이썬 사용 scala도 사용

주요 생성/아이디어 : Resilient Distributed Dataset (RDD)

---
RDD란?

분산되어 저장되는 데이터 셋
(key-value 형태를 일반화한것)

RDD에서는 key-value 형태를 강제하지 않는다.

Spark는 최대한 디스크를 안쓰려고하고 Caching 데이터를 통해 진행됨

RDD는 Hadoop이나 다른 RDD 변환 시스템으로 부터 생성된다.

---
Spark RDD Opertation

Transfromations : RDD를 만들어냄
- Transformation은 map, filter, join, union, intersection, distinct 가 포함되어있다.
- Lazy evaluation <> eager evaluation
스파크는 사용자의 작용이 없으면 수행하지 않는 Lazy evaluation을 사용[기타 다른 프로그램과 다른 모습]
(다른 프로그램은 출력의 작용이 있기전까지 연산을 해놓은 상태로 기다리지만 스파크는 출력 명령을 받고
연산을 시작함)

Action: 값과 데이터를 반환
- Action은 count, collect, reduce, save를 포함한다.

---
DAGS

방향성이 있고 사이클이 없어 역행하지 않는 그래프를 사용

- 일반적인 작업 그래프를 지원
- 파이프 라인을 사용
- 캐시를 쓸 수 있음(지역성, 재사용)
- 셔플의 병목현상을 피하도록 파티션을 사용한다.

---
DataFrame 
- 열과 행으로 이루어진 관계 데이터베이스 테이블

Dataset
- type-sage, 객체지향 프로그래밍 인터페이스

모두 Spark SQL 엔진에서 구성되며, RDD로 전환될 수 있다.

---
Useful Libraires from Spark 

Spark SQL

spark streaming - stram processing of live datastream
MLib - scalable machine learning
GraphX - graph manipulation

---
Spark vs Hadoop MapReduce

성능상으로는 스파크가 빠름

스파크는 메모리에서 data 접근 가능

MapReduce 는 단순하기때문에 다른 서비스들과 잘 작동

Spark는 메모리를 왕창 잡아먹어서 다른 서비스와 잘 작동이 안될 수 있음

프로그래머의 입장에서는 쉬운건 Spark (하이레벨 API)

---
Problems Suited for MapReduce

Example : Host size
우리가 큰 web corpus 를 가지고 있다고 가정해보자
metadata file을 보자 (URL, size, date,... ) 무수히 많은 데이터

각 호스트에게, 바이트의 최종 수를 찾는다
즉, 개인이 호스트하는 URL form page size의 합계

Example : Language Model
통계적 기계 번역 :
큰 corpus의 문서에서 문장이 발생할 때 5번째 단어마다 카운트를 해줘서 숫자를 세준다.

Example : Join By Map-Reduce
R(A,B) 와 S(B,C)를 조인해주는 것을 연산
각각의 파일을 저장해야함
(a,b) or (b,c) 쌍의 튜플

A  B     	 B  C 	A C
a1 b1  	 b2 c1	a3 c1
a2 b1 조인 b2 c2  =  a3 c2
a3 b2  	 b3 c3     	a4 c3
a4 b3
[같은거 끼리 묶어준다]
Map에서 key 값으로 B값을 써준다.
구분을 위해 속성값을 넣어줌
(b1, (a1, R))	(b2, (c1, S))	b1으로 묶이는거 없음
(b1, (a2, R))	(b2, (c2, S))	b2 -> a3 c1, c3 c2
(b2, (a3, R))	(b3, (c3, S))	b3 -> a4 c3
(b3, (a4, R))

-- 이렇게 말고 hash function을 사용할 수도 있음
[hash function h from B-value to 1...k]
Map 진행과정 :
각각은 입력한다 tuble R(a,b)를 key-value pair (b,(a,R))로
해시함수로 key를 넣은다음에 MapReduce 작업을 한다.

MapReduce는 랜덤하거나 불규칙한 데이터에 접근이 요구되는 문제에는 충분하지 않다.
- 그래프
- 상호독립적 데이터
 > 머신 러닝
 > 여러 페어의 아이템의 비교

---
input =>mappers =>[group by key] => reducers => output
---
맵리듀스 알고리즘의 비용을 측정하는 척도

1. Communication Cost = 모든 프로세스에서 일어난 I/O 작업
2. Elapsed comunnication cost = 모든 I/O 패스중 가장 큰 것
3. (Elapsed) computation cost = 모든 프로세스들의 총 실행 시간
* big-O notation은 효용성이 없다.

---
Spark : RDD

맵 리듀스나 Spark을 데이터 플로우 시스템으로 볼 수 있다.

데이터 플로우 : 데이터가 흘러갈 수 있는 길

데이터 플로우 처리 알고리즘이 2개가 있을 때
같은 치리 속도(양)의 결과가 나올때 [모든 프로세스에서 일어난 I/O 작업]
각 과정에서 어느정도의 처리양이 분산되어있는지가 중요하다. [모든 패스에서 일어난 각 작업]

---
Cost Measures
- Communication cost = 인풋 파일 사이즈 + 2x(맵 프로세스에서 리듀스 프로세스로 가는 파일 사이즈의 합) [그룹 바이 키때문에]
+ 리듀스 프로세스에서 출력된 사이즈의 합

- (Elapsed)Communication cost[(흘러간)] = 가장 큰 input + any 맵 프로세스의 출력 + 같은 것 any 리듀스 프로세스

---
What cost measure mean

-입출력이나 계산 둘중에 하나가 비용 측정을 결정한다 [둘 다 고려할 필요 x] [상황에 따라 바꿔가면서 사용]
-여러분이 사용하고 있는 클라우드 컴퓨팅에 지불하고 있는 비용을 나타낸다. (직접 서버 구동 vs 클라우드)(수지타산 비교)
- 흘러간 비용[elapsed cost]는 병렬화를 사용하는 벽시계 시간

---
map-reduce join의 비용
- total communication cost
= O(|R|+|S|+|R조인S|) - Big O는 대충 계산하는거
- Elapsed communication cost = O(s)
* Big O 관련내용은 시험에 내지 않을 겁니다

---
데이터에서 자주 등장하는 패턴 찾기 & 연관 법칙 찾기
---
연관 법칙 탐색

슈퍼 마켓에서 충분히 많은 고객들이 같이 사는 물건 찾기

"영수증을 봤을 때 A를 사는 사람들은 보통 B를 같이 사더라~"

---
장바구니 모델

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

출력:
규칙 발견 : 우유를 사면 콜라도 산다 / 기저귀, 우유를 사면 맥주도 산다

- 상당히 많은 아이템
- 상당히 많은 장바구니 내역(비교적적은 아이템 내용)

> 연관관계 발견
{x,y,z}를 사는 사람은 {v,w}를 사는 경향이 있다.

---
일반화

아이템/장바구니 > 단어/문서
아이템/장바구니 > 약/환자

* 꼭 장바구니에 한정되어서 사용되는 모델은 아니다
[아이템을의 연관관계를 찾기 위해 사용한다]
---
아이템/장바구니
문장/문서
환자/약&부작용

> 유의미한 연관관계를 찾아내 긍정적인 효과를 위해 사용할 수 있다.

---
개요

First : 정의
빈번한 아이템셋은 무엇인가?
연관관계는 무엇인가? : Confidence(신뢰도/확신), Support(지원), interestingness(흥미)

Second(Then) : 빈번한 아이템셋 찾기 알고리즘
- Finding frequent pairs 빈번한 쌍 찾기
- A-Priori algorithm 선행 알고리즘 / 빈발항목집합(frequent itemsets) 및 연관규칙분석을 위한 알고리즘
- PCY algorithm A-Priori 알고리즘에서, 첫번째 패스의 사용되지 않는 메모리 공간을 활용하는 알고리즘

---
빈발항목집합(frequent itemsets)

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

Support : 해당 아이템 셋을 품고 있는 장바구니를 찾아라!

Support of {맥주,빵}
>> 구매내역에서 맥주와 빵을 둘다 포함하는 장바구니 찾기 >> 2, 4
Support of {맥주,빵} = 2  || [2/5  = 0.4] < 이렇게 표기하기도함

Support threshold (s) : 이정도면 빈번하게 등장하는 구나라고 정의하는 한계점
Support 가 이 s 를 넘으면 빈번한 아이템셋으로 셀 수 있음

---
Association Rules (연관 규칙)

장바구니에 우유가 있으면 맥주가 있을 확률이 높다.
장바구니에 x가 있으면 y가 있을 확률이 높다.

> 이러한 규칙을 찾아내는 것 (연관있는, 흥미로운 규칙을 찾아내기)

Confidence(신뢰도/확신)은 I의 집합에 대해서 j에 대한 신뢰도를 나타냄

conf(I -> j) = support(I U j) / support(I)
> 여러가지 아이템들의 집합에서 J가 등장할때의 신뢰도는 해당 아이템이 등장한 횟수 중 I와 j가 같이 등장한 횟수

구매내역
1. 빵, 콜라, 우유
2. 맥주, 빵
3. 맥주, 콜라, 기저귀, 우유
4. 맥주, 빵, 기저귀, 우유
5. 콜라, 기저귀, 우유

우유 ->콜라 의 신뢰도(confidence)
=  콜라와 우유의 support(콜라와 우유가 같이 등장한 횟수)/우유의 support(우유가 등장한 횟수) 
= 3/4 = 0.75

---
흥미로운 연관 관계
> 높은 신뢰도를 가진다고만 해서 흥미로운 연관관계는 아니다.
[칫솔을 살때 치약을 산다고해서 흥미로운 연관관계는 아니다.]
[예상하기 힘든, 예측이 어려운 결과가 나타난 다면 흥미로운 연관관계라고 할 수 있다.]

> x를 살때 y를 산다는 연관관계가 있다면
Interest(x -> y) = |conf(x->y) - Pr[y]|  *Pr[y]가 등장할 확률
x를 살때 y를 사는 것이 흥미로운가 = (x -> y의 신뢰도 - y가 등장할 확률)의 절댓값 [y가 등장할 확률이 더 클 수 있어서]

---
B1 = m, b, c
B2 = m, p, j
B3 = m, b
B4 = c, j
B5 = m, p, b
B6 = m, c, b, j
B7 = c, b, j
B8 = b, c

연관관계 : {m,b} -> c
Support = 2  	cuz [B1, B6]
Confience = support({m,b}->c) / support({m,b}) = 2/ 4 	cuz [B1,B6] / [B1, B3, B5, B6] 
Interest = | conf({m,b}->c) - pr[c] | = | 2/4 - 5/8 | = | 4/8 - 5/8 | = 1/8  cuz pr[c] => 전체 아이템셋중 c가 들어있는 것

---
연관 규칙을 찾아낼수 있는 mining

support >= s & confidence >= c
(빈번히 등장하는 한계점을 넘고 신뢰성에 대한 한계점을 넘을 때)

* 빈번한 아이템셋을 찾을 때 모든 경우의 수를 세는 것은 굉장한 시간 소요가 필요할 것이다. 
[ 이 방법에 대해서는 나중에 설명할 것이다. ]

Step1 : 모든 frequenct itemset 찾기 [ I 찾아냄]
Step2 : 규칙 일반화(Rule generation)

I의 부분집합인 모든 A에 대해서 A -> I | A 규칙을 일반화함

variant1:
모든 조합중 confidence 가 confidence 한계점 c 보다 높은 것을 찾아냄
variant2:
관측(Obsevation) : 만약 A,B,C -> D 가 신뢰도 밑이면, A,B -> C,D 도 마찬가지이다.
작은 것으로 부터 "bigger" 규칙을 일반화한다
큰 것에 대한 부분집합에 대해서 신뢰도가 나오면 그보다 작은 부분집합은 계산할 필요가 없다.

B1 = m, b, c
B2 = m, p, j
B3 = m, b
B4 = c, j
B5 = m, p, b
B6 = m, c, b, j
B7 = c, b, j
B8 = b, c

Support 한계점 s = 3, 신뢰도 한계점 c = 0.75 (3/4)
Step1 ) 빈번아이템셋 찾기 :
{b,m}, {b,c}, {c,m}, {c,j}, {m,c,b}
Step2 ) 일반화 규칙
b -> m : c = 4/6 X 
m -> b : c = 4/5
b -> c : c = 5/6
c -> b : c = ...
... (c -> m ....)
b,c -> m : c = 3/5 X
b,m -> c : c = 3/4
b -> c,m : c = 3/6 X

신뢰도가 낮은것은 그것에 대한 부분집합으로 이루어진 것은 계산하지 않아도 충분히 낮을 거라는 것에 대한 예측이 가능

---
Compacting the Output  (압축)
규칙들의 수를 줄이기
Maximal 빈번 아이템 / 방법
또는
Closed itemsets / 방법

Maximal 은 support 의 한계점 을 이용( 충분히 등장하느냐?)
Support 를 s에 대해서 
먼저 더 자세히한 집합이 있는가(A이면 AB, AC 같은 것이 있는가?)
있으면 그쪽으로 가서 Maximal을 넘는지 비교
AB가 maximal을 넘으면 A는 No
더 자세한 것이 없고 A가 maximal을 넘으면 Yes
[No 인 것들은 압축할때 지워진다]

Closed
자기보다 더 큰 무엇인가가 그 숫자를 보존하고 있는지
A면 AB,AC 같은 것들이 A보다 작은지
AB,AC가 A보다 같거나 크면 A는 No
만약 A가 가장 크면 Yes

	Support	Maximal(s=3)	Closed
A	4	No		No
B	5	No		Yes
C	3	No		No
AB	4	Yes		Yes
AC	2	No		No
BC	3	Yes		Yes
ABC	2	No		Yes

* Maximal로 압축할지/ Closed로 압축할지는 결정의 영역

---
Naive Algorithm / 순진한(naive) 알고리즘

Computation Model 연산 모델
데이터는 데이터 시스템에 깔끔히 존재하는 것이 아니라 일반적인 텍스트같은 flat 파일에 저장되어있다.

[1,2,3,4,5,-1,1,4,5,-1 ...]   * [-1]은 장바구니의 끝

디스크 I/O의 숫자를 세는것이 cost
일반적으로 데이터가 이동하는 것인
pass의 숫자로 cost를 센다.

각 아이템이 얼마나 많이 등장했는지 세고싶은데 main-memory의 능력이 모든것을 셀정도의 동작을 하기는 힘들다.
> Main-Memory Bottleneck
> 모든 pair를 세는 것만으로도 엄청나게 많은 메모리가 필요하다.

아이템이 10만개만 되어도 20기가의 메인메모리가 필요하다

> 디스크에 저장하고 버리고 저장하고 버리고하는 스와핑은 재앙을 불러일으킬수 있다.

---
Frequent itemsets 을 찾는것에 먼저 Frequent Pairs 를 먼저 찾아보자
{i1, i2} 같은 페어만 찾기
> 나중에 확장해서 itemset 에도 적용

way1. 
가능한 모든 조합의 쌍을 생성해서 세어보기 
items = {a, b, c, d, e }
a, b|a,c|a,d|a,e|b,c|b,d|,b,e|c,d|c,e|d,e|
의 쌍을 세어놓고
빈번 아이템쌍에서 몇번 등장하는지 전부 세어준다.

> 메인메모리에 과부하가 발생할 확률이 높다
> 필요하지 않은 쌍을 위한 불필요한 공간을 만든다.

way2
세가지의 테이블을 만든다. 
"[i,j,c] = {i,j} 쌍의 count는 c 이다"
{i,j}]의 c까지 담은 triple을 만들어 저장

triangular matrix = 모든 쌍 저장 [불필요한 데이터 저장] 		> way1
tripless(item i, item j, count) = 유의미한 쌍과 count만 저장		> way2
*삼각형으로 그리는 이유는 순서가 바뀌는 경우는 중복쌍이기 때문에

way2에서는 해쉬함수를 이용하면 좀 더 효율적으로 저장할 수 있다.
> 실제 사용하는 메모리는 더 커질 수가 있다. 3개의 데이터를 저장해야되기 떄문에
> 그렇기 때문에 A-Priori Algorithm 알고리즘의 필요가 나온다.
---
A-Priori Algorithm 선행적 알고리즘

two-pass 접근

naive 에서는 one-pass 로 한번 읽고 끝이지만 A-Priori Algorithm 에서는 두번 읽는다.
> 속도는 느리긴하지만 main memory 사용량은 줄어들게 된다.

Key idea : monotonicity 단조성
> I라는 아이템의 집합이 최소 s번 등장한다면 , I의 J 부분 집합도 항상 그럴 것이다.
대우 명제 :
a가 10번이상 안나왔다면 a를 포함하고 있는 모든 쌍도 10번이상 나오지 않았을것이다.

* a 를 세어보니깐 10번 등장한다. a를 포함하는 부분집합도 절대 10번을 넘을 수 는 없을 것이다.
> 즉, 셀 필요가 없는 것들을 지울수 있는 것이다.

그렇다면, A-Priori 는 빈번한 쌍을 어떻게 찾겠다는걸까?

두번의 pass를 통해 찾을 것이다.

Pass1 : 각 개별 아이템들이 몇번씩 등장했는지 셀 것이다.
[아이템 수에 비례하는 메모리만 필요할 것이다.] / 공간은 O(n) 만큼 들겠지? 효율적이다.
a가 10번 등장하면 a와 pair 를 이루는 것들은 절대 10번은 등장할 수 없겠다.
이것을 바탕으로 간소화 한다.

Pass2: pass1에서 아이템의 사이즈를 줄였고 다시 장바구니를 돌아가면서 빈번한 요소들을 가지고있는
쌍들만 찾아가면서 세어본다. [naive 알고리즘으로 찾기]
* pass1에서 frequent item을 찾아서 저장해야한다. [개별 아이템을 기준으로]

Detail for A-Prioir
- triangular matrix 방법을 사용할 수 있다. 빈(번한 아이템의 숫자인 n을 갖고)
- 공간을 저장해라 저장된 (i,j,c) 같은 트리플을 갖고
- Trick: 
다시 넘버링 해라 1,2.. 를 그리고 테이블을 새로운 넘버와 원래 넘버와 연관시켜라

Frequent Triples
빈번하게 등장하는 k에 대해서 k-tuples을 찾고싶다면(size k의 집합)
(k-1 까지 빈번한 아이템을 찾았다는 가정하에) pair를 찾았다는 가정

Ck = candidate k-tuples (빈번할만한 k 튜플 후보)
Lk = 빈번한 k-tuple의 진정한 집합

C1 -> Filter -> L1 Construct -> C2 -> Fileter -> L2 -> Construct -> C3
All item -> Count the item -> All pairs of items from L1 -> * -> Count the pairs -> * -> To be explained

---
Hypothetical steps of the A-Priori algorithm A-Priori (알고리즘의 가상의 단계)
▪ C1 = { {b} {c} {j} {m} {n} {p} }		* 개별 아이템들
▪ Count the support of itemsets in C1		* 개별 아이템들의 support 를 세봤다.
▪ Prune non-frequent. We get: L1 = { b, c, j, m }	* 빈번하지 않는 조합들 가지치기 후 빈번한 개별아이템 L1 집합
▪ Generate C2 = { {b,c} {b,j} {b,m} {c,j} {c,m} {j,m} }	* 가능한 모든 조합의 일반화 C2
▪ Count the support of itemsets in C2		* C2 의 support 세보기
▪ Prune non-frequent. L2 = { {b,m} {b,c} {c,m} {c,j} }	* 빈번하지 않는 조합들 가지치기 후  빈번한 pair 아이템 L2 집합
▪ Generate C3 = { {b,c,m} {b,c,j} {b,m,j} {c,m,j} }	* 가능한 모든 트리플 조합의 일반화 C3
▪ Count the support of itemsets in C3		* C3의 support 세보기
▪ Prune non-frequent. L3 = { {b,c,m} }		* 빈번하지 않는 조합들 가지치기 후  빈번한 트리플 아이템 L3 집합
---
한번씩 진행될때마다 itemset 사이즈의 k를 한번 pass해야함
후보의 k-tuple을 셀 수 있는 메인메모리 필요
어늦어도 합리적인 서포트에서는 1%이상 등장하면 빈번한것이다.
k = 2에서 가장 많은 메모리를 잡아먹는다.

Many possible extensions:
▪ Association rules with intervals:	간격과 연관 관계
▪ For example: Men over 65 have 2 cars	65세를 넘는 남자는 2개의 차가 있다.
▪ Association rules when items are in a taxonomy	한 분류에 아이템이 있을 때의 연관 관계 
▪ Bread, Butter → FruitJam			빵,버터 -> 과일잼
▪ BakedGoods, MilkProduct → PreservedGoods	구워진제품,우유제품 -> 제공되는 물품
▪ Lower the support s as itemset gets bigger	itemset 으로써의 낮은 support s 는 커진다.

---
PCY(Park-Chen-Yu) 알고리즘	[선언적 알고리즘보다 3배정도 빠르다]

Pass 1 :
선행적 알고리즘과 동일하게 개별 아이템을 돌아보면서 아이템의 숫자에 1을 더해가면서 카운팅을 해준다.
+ (PCY 의 추가적인 루틴) 
장바구니 속에 존재하는 모든 아이템 페어에 대해서 해쉬로 1을 더해가며 카운팅을 해준다.
* 모든 개별적인 아이템 카운팅, 해쉬를 이용해서 모든 pair 에 대해 카운팅(모든 독립적인 공간을 보장하진X)

Pass 2:
오직 hash에서 빈번한 버킷들의 pair들을 세어준다.

Observation: 
만약 bucket이 빈번한 쌍인 경우, 버킷은 무조건 빈번하다.
하지만, 빈번하지 않은 것들의 쌍을 제외하더라도 한 버킷은 빈번하다.
따라서 우리는 사용할 수 없다 해쉬를 빈번한 버킷의 어떤 멤버들도 제거할 수 없다.
하지만, s보다 작은 버킷들은 아무것도 빈번할 수 없다.
이 버킷을 해쉬하는 쌍은 2개의 빈번한 아이템의 쌍의 후보로부터 제거될 수 있다.

---
PCY 알고리즘은 A-Priori 알고리즘에서, 첫번째 패스의 사용되지 않는 메모리 공간을 활용하는 알고리즘

백만개의 아이템이 있고 메모리 용량이 GB 단위라면,
첫번째 패스에서 메모리 공간은 전체의 약 10% 만 사용됩니다.
첫번째 패스에서 메모리는 item 이름을 정수로 변환하기 위한 테이블(translation table)과, 
그 정수(아이템) 의 카운트를 저장한 테이블, 이 두개의 공간만 필요합니다.

FOR (each basket) {
   FOR (each item in the basket)
    add 1 to item’s count;
   FOR (each pair of items) {
    hash the pair to a bucket;
    add 1 to the count for that bucket
   }
}
아이템들의 pair는 해시 테이블의 버킷으로 해시됩니다.
첫번째 패스에서 바스켓을 검사하면서, 
각 바스켓에 있는 아이템의 카운터에 1을 더하고(A-Priori), 모든 pair를 생성합니다.

생성된 각 pair를 해시하고 pair가 해시된 버킷에 1을 더합니다. 
여기서 pair 자체가 버킷에 들어가는 것이 아닙니다. pair는 버킷의 단일 정수에게만 영향을 끼칩니다.


첫번째 패스가 끝나면, 각 버킷은 카운트를 갖습니다.
이 카운트 그 버킷으로 해시된 모든 pair들의 카운트의 합입니다.

한 버킷의 카운트가 s 보다 크다면, 이것을 ‘frequent bucket’ 이라고 합니다.

frequent bucket 으로 해시되는 pair 를 그대로 빈번하다고 볼 수는 없습니다.

그러나 버킷이 s 이하로 카운트 된다는 것은, 그 버킷에 빈번해질 수 있는 pair가 없다고 봅니다.
여기서 그 pair의 아이템들이 모두 빈번하더라도 pair 에 대해 수행하므로,
 pair 자체의 빈도에 대한 정보를 얻을 수 있는 것입니다.

이 정보는 두번째 패스 때 활용됩니다. 후보 pair C2 에 대한 정의를 다음과 같이 바꿀 수 있습니다.

PCY에서 pair 의 카운터를 저장하는 경우 삼각0행렬을 사용하지 않습니다.

A-Priori 에서는 두번째 패스에서 삼각행렬을 사용하는 것이 가능했지만
(1-m 으로 리넘버링 하면서), PCY 에서는 쓸 수 없습니다.
빈도 아이템들로 만들어낸 pair 들은 삼각행렬안에 있는 임의로 위치합니다.
이 중에는 첫번째 패스에서 frequent bucket으로 해시되지 않은 pair 가 있을 수 있습니다.
1~m 으로 리넘버링된 행렬에는 후보가 아닌 pair가 포함되어 있으므로 
해시로 버킷을 만드는 작업의 의미가 없어집니다.

PCY 에서는 triple 방식을 사용합니다.
이 제약은 빈번한 아이템이 많이 나오지 않는 상황에는 괜찮지만, 
많은 pair들이 빈번하게 나오는 경우는 비효율적입니다.
그래서 PCY 에서, C2 에 빈도 아이템의 pair의 2/3 이상 줄여주지 않는다면
A-Priori 대신 PCY 를 쓰는 이점이 없어집니다.
:Triple을 사용할 때, 실제 pair 가 전체의 1/3 이하인 경우에 삼각행렬 보다 효율적입니다.

PCY로 frequent pair을 찾는 작업은 A- Priori와 크게 다르지만, 
frequent triple 이나 더 큰 세트를 찾는 단계는 본질적으로 A-Priori와 동일합니다.

---
2 pass 이하로 읽어서 빈번한 아이템셋을 찾을 수 있을까?
2 혹은 그보다 낮은 pass 로 찾는 접근법
-Random sampling
-Son
-Toivonen 

---
Random sampling

장바구니가 너무 크서 메모리에 안들어가니깐 랜덤 샘플링을 해서 그걸로 작업한다.

랜덤 샘플링이 1/10 이라면 threshold도 1/10 해줘야 한다.

랜덤 샘플링을 통해 가져온 샘플링에는 false positive가 있을 수도 있다.
(양성반응이 나왔는데 가짜 양성반응이다)
> 이게 걱정된다면 두번째 pass 를 진행해서 확인을 할 수도 있다.

* son 알고리즘은 random sampling 의 단점에 대해 다룬다
---
